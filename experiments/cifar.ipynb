{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from admm.agents import FedConsensus\n",
    "from admm.servers import EventADMM\n",
    "from admm.models import FCNet, CNN\n",
    "from admm.utils import average_params\n",
    "from admm.data import partition_data, split_dataset\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "cifar_trainset = datasets.CIFAR10(\n",
    "    root='./data/cifar10', train=True,\n",
    "    download=True, transform=cifar_transform\n",
    ")\n",
    "cifar_testset = datasets.CIFAR10(\n",
    "    root='./data/cifar10', train=False,\n",
    "    download=False, transform=cifar_transform\n",
    ")\n",
    "\n",
    "mnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)), transforms.Lambda(lambda x: torch.flatten(x))\n",
    "])\n",
    "\n",
    "mnist_trainset = datasets.MNIST(\n",
    "    root='./data/mnist_data', train=True,\n",
    "    download=True, transform=mnist_transform\n",
    ")\n",
    "mnist_testset = datasets.MNIST(\n",
    "    root='./data/mnist_data', train=False,\n",
    "    download=False, transform=mnist_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 0 distribution: [526. 489. 563. 518. 551. 569. 534. 551. 577. 543.] - num_samples = 5421.0\n",
      "Dataset 1 distribution: [544. 555. 535. 543. 538. 519. 567. 523. 566. 531.] - num_samples = 5421.0\n",
      "Dataset 2 distribution: [493. 537. 549. 573. 544. 561. 551. 528. 537. 548.] - num_samples = 5421.0\n",
      "Dataset 3 distribution: [554. 529. 549. 536. 554. 536. 508. 509. 573. 573.] - num_samples = 5421.0\n",
      "Dataset 4 distribution: [571. 514. 565. 538. 530. 567. 530. 540. 533. 533.] - num_samples = 5421.0\n",
      "Dataset 5 distribution: [536. 561. 562. 549. 527. 516. 553. 547. 520. 550.] - num_samples = 5421.0\n",
      "Dataset 6 distribution: [567. 592. 504. 552. 558. 519. 526. 566. 529. 508.] - num_samples = 5421.0\n",
      "Dataset 7 distribution: [562. 570. 527. 519. 559. 563. 561. 559. 490. 511.] - num_samples = 5421.0\n",
      "Dataset 8 distribution: [544. 540. 509. 517. 535. 545. 565. 586. 539. 541.] - num_samples = 5421.0\n",
      "Dataset 9 distribution: [524. 534. 558. 576. 525. 526. 526. 512. 557. 583.] - num_samples = 5421.0\n",
      "Validation dataset 9 distribution: [1171. 1334. 1184. 1264. 1182. 1074. 1172. 1305. 1155. 1159.] - num_samples = 12000.0\n",
      "torch.Size([1, 784])\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset, _ = split_dataset(dataset=mnist_trainset, train_ratio=0.8, val_ratio=0.2)\n",
    "\n",
    "trainsets = partition_data(\n",
    "    num_clients=10,\n",
    "    iid=True,\n",
    "    balance=True,\n",
    "    power_law=True,\n",
    "    seed=42,\n",
    "    trainset=train_dataset.dataset,\n",
    "    labels_per_partition=10\n",
    ")\n",
    "\n",
    "for i, dataset in enumerate(trainsets):\n",
    "    labels = np.zeros(10)\n",
    "    loader = DataLoader(dataset, batch_size=1)\n",
    "    for data, target in loader:\n",
    "        labels[target.item()] += 1\n",
    "    print(f'Dataset {i} distribution: {labels} - num_samples = {labels.sum()}')\n",
    "\n",
    "labels = np.zeros(10)\n",
    "loader = DataLoader(val_dataset, batch_size=1)\n",
    "for data, target in loader:\n",
    "    labels[target.item()] += 1\n",
    "print(f'Validation dataset {i} distribution: {labels} - num_samples = {labels.sum()}')\n",
    "\n",
    "oader = DataLoader(val_dataset, batch_size=10)\n",
    "for data, target in loader:\n",
    "    print(data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loaders = [DataLoader(dataset, batch_size=batch_size, shuffle=True) for dataset in trainsets]\n",
    "test_loader = DataLoader(mnist_testset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Comm frequency: 1.000, agent 0: 0.86, agent 1: 0.85, agent 2: 0.86, agent 3: 0.86, agent 4: 0.88, agent 5: 0.85, agent 6: 0.87, agent 7: 0.87, agent 8: 0.86, agent 9: 0.86: 100%|██████████| 70/70 [23:34<00:00, 20.21s/it]\n",
      "Comm frequency: 1.000, agent 0: 0.20, agent 1: 0.21, agent 2: 0.33, agent 3: 0.20, agent 4: 0.31, agent 5: 0.15, agent 6: 0.37, agent 7: 0.25, agent 8: 0.34, agent 9: 0.21:   7%|▋         | 5/70 [01:57<25:26, 23.49s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/michaelcummins/ADMM/experiments/cifar.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/michaelcummins/ADMM/experiments/cifar.ipynb#W6sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# Run consensus algorithm\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/michaelcummins/ADMM/experiments/cifar.ipynb#W6sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m server \u001b[39m=\u001b[39m EventADMM(clients\u001b[39m=\u001b[39magents, t_max\u001b[39m=\u001b[39mt_max)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/michaelcummins/ADMM/experiments/cifar.ipynb#W6sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m server\u001b[39m.\u001b[39;49mspin(loader\u001b[39m=\u001b[39;49mval_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/michaelcummins/ADMM/experiments/cifar.ipynb#W6sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# For plotting purposes\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/michaelcummins/ADMM/experiments/cifar.ipynb#W6sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m acc_per_delta[i,:] \u001b[39m=\u001b[39m server\u001b[39m.\u001b[39mval_accs\n",
      "File \u001b[0;32m~/ADMM/admm/servers.py:31\u001b[0m, in \u001b[0;36mEventADMM.spin\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m     29\u001b[0m         agent\u001b[39m.\u001b[39mprimal_update(overfit)\n\u001b[1;32m     30\u001b[0m     \u001b[39melse\u001b[39;00m: \n\u001b[0;32m---> 31\u001b[0m         agent\u001b[39m.\u001b[39;49mprimal_update()\n\u001b[1;32m     32\u001b[0m overfit \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m# Test updated params on validation set\u001b[39;00m\n",
      "File \u001b[0;32m~/ADMM/admm/agents.py:106\u001b[0m, in \u001b[0;36mFedConsensus.primal_update\u001b[0;34m(self, overfit)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprimal_update\u001b[39m(\u001b[39mself\u001b[39m, overfit\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m \n\u001b[1;32m    104\u001b[0m     \u001b[39m# Solve argmin problem\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iters):\n\u001b[0;32m--> 106\u001b[0m         \u001b[39mfor\u001b[39;00m data, target \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_loader:\n\u001b[1;32m    107\u001b[0m             data, target \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice), target\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    108\u001b[0m             out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(data)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/disop/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/disop/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/disop/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset, \u001b[39m\"\u001b[39m\u001b[39m__getitems__\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset\u001b[39m.\u001b[39;49m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/disop/lib/python3.10/site-packages/torch/utils/data/dataset.py:362\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitems__\u001b[39m(\u001b[39mself\u001b[39m, indices: List[\u001b[39mint\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[T_co]:\n\u001b[1;32m    359\u001b[0m     \u001b[39m# add batched sampling support when parent dataset supports it.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m     \u001b[39m# see torch.utils.data._utils.fetch._MapDatasetFetcher\u001b[39;00m\n\u001b[1;32m    361\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset, \u001b[39m\"\u001b[39m\u001b[39m__getitems__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)):\n\u001b[0;32m--> 362\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset\u001b[39m.\u001b[39;49m__getitems__([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m indices])  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    364\u001b[0m         \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[idx]] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/disop/lib/python3.10/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices])  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[idx]] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/disop/lib/python3.10/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices])  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/disop/lib/python3.10/site-packages/torch/utils/data/dataset.py:302\u001b[0m, in \u001b[0;36mConcatDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     sample_idx \u001b[39m=\u001b[39m idx \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcumulative_sizes[dataset_idx \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[0;32m--> 302\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdatasets[dataset_idx][sample_idx]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/disop/lib/python3.10/site-packages/torch/utils/data/dataset.py:356\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 356\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/disop/lib/python3.10/site-packages/torchvision/datasets/mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/disop/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/disop/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/disop/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/disop/lib/python3.10/site-packages/torchvision/transforms/transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, tensor: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    270\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[39m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mnormalize(tensor, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmean, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstd, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/disop/lib/python3.10/site-packages/torchvision/transforms/functional.py:363\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(tensor, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    361\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimg should be Tensor Image. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensor)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 363\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39;49mnormalize(tensor, mean\u001b[39m=\u001b[39;49mmean, std\u001b[39m=\u001b[39;49mstd, inplace\u001b[39m=\u001b[39;49minplace)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/disop/lib/python3.10/site-packages/torchvision/transforms/_functional_tensor.py:922\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    920\u001b[0m mean \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(mean, dtype\u001b[39m=\u001b[39mdtype, device\u001b[39m=\u001b[39mtensor\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    921\u001b[0m std \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(std, dtype\u001b[39m=\u001b[39mdtype, device\u001b[39m=\u001b[39mtensor\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 922\u001b[0m \u001b[39mif\u001b[39;00m (std \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49many():\n\u001b[1;32m    923\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstd evaluated to zero after conversion to \u001b[39m\u001b[39m{\u001b[39;00mdtype\u001b[39m}\u001b[39;00m\u001b[39m, leading to division by zero.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    924\u001b[0m \u001b[39mif\u001b[39;00m mean\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rates = [0.005, 0.01, 0.02]\n",
    "rho = 0.01\n",
    "delta = 0\n",
    "t_max = 150\n",
    "# loaders = [digit_1_train_loader_fc, digit_2_train_loader_fc]\n",
    "loaders = train_loaders\n",
    "device = 'cpu'\n",
    "\n",
    "acc_per_delta = np.zeros((len(rates), t_max))\n",
    "rate_per_delta = np.zeros((len(rates), t_max))\n",
    "loads = []\n",
    "test_accs = []\n",
    "\n",
    "print(f'N = {len(loaders)}')\n",
    "for i, lr in enumerate(rates):\n",
    "    \n",
    "    agents = [\n",
    "        FedConsensus(\n",
    "            N=len(loaders),\n",
    "            delta=delta,\n",
    "            rho=rho,\n",
    "            model=FCNet(in_channels=784, hidden1=200, hidden2=None, out_channels=10),\n",
    "            loss=nn.CrossEntropyLoss(),\n",
    "            train_loader=loader,\n",
    "            classification=True,\n",
    "            epochs=2,\n",
    "            device=device,\n",
    "            lr=lr\n",
    "        ) for loader in loaders\n",
    "    ]\n",
    "\n",
    "    # Broadcast average to all agents and check if equal\n",
    "    for agent in agents:\n",
    "        agent.primal_avg = average_params([agent.model.parameters() for agent in agents])\n",
    "    for param1, param2 in zip(agents[0].primal_avg, agents[1].primal_avg):\n",
    "        if not torch.equal(param1, param2): raise ValueError(\"Averaged params aren't equal\")\n",
    "\n",
    "    # Run consensus algorithm\n",
    "    server = EventADMM(clients=agents, t_max=t_max)\n",
    "    server.spin(loader=val_loader)\n",
    "    \n",
    "    # For plotting purposes\n",
    "    acc_per_delta[i,:] = server.val_accs\n",
    "    rate_per_delta[i,:] = server.rates\n",
    "    loads.append(server.comm)\n",
    "    accs = server.validate(loader=test_loader)\n",
    "    test_accs.append(sum(accs)/len(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = range(t_max)\n",
    "# Plot accuracies\n",
    "for acc, delta in zip(acc_per_delta, rates):\n",
    "    plt.plot(T, acc, label=f'rate={delta}')\n",
    "plt.legend(loc='center right', bbox_to_anchor=(1.3, 0.5))\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Set Accuracy - Fully Connected - Learning Rate = 0.001')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rate, delta in zip(rate_per_delta, rates):\n",
    "    plt.plot(T, rate, label=f'rate={delta}')\n",
    "plt.legend(loc='center right', bbox_to_anchor=(1.3, 0.5))\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Rate')\n",
    "plt.title('Communication Rate - Fully Connected')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for load, acc, delta in zip(loads, test_accs, rates):\n",
    "    plt.plot(acc, load, label=f'rate={delta}', marker='o')\n",
    "plt.legend(loc='center right', bbox_to_anchor=(1.3, 0.5))\n",
    "plt.xlabel('Test Error')\n",
    "plt.ylabel('Communication Load')\n",
    "plt.title('Fully Connected')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training partitions\n",
      "\n",
      "Partition 0\n",
      "[1705, 1605, 1682, 1628, 1669, 1683, 1674, 1692, 1643, 1686]\n",
      "\n",
      "Partition 1\n",
      "[1599, 1723, 1659, 1729, 1670, 1647, 1679, 1644, 1653, 1664]\n",
      "\n",
      "Partition 2\n",
      "[1696, 1672, 1659, 1643, 1661, 1670, 1647, 1664, 1704, 1650]\n",
      "\n",
      "Test partition\n",
      "\n",
      "[1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]\n",
      "\n",
      "Validation partition\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The given split: 'val' is not present in the dataset's splits: '['train', 'test']'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/michaelcummins/ADMM/experiments/cifar.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/michaelcummins/ADMM/experiments/cifar.ipynb#W1sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mprint\u001b[39m(b)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/michaelcummins/ADMM/experiments/cifar.ipynb#W1sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mValidation partition\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/michaelcummins/ADMM/experiments/cifar.ipynb#W1sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m partition \u001b[39m=\u001b[39m fds\u001b[39m.\u001b[39;49mload_partition(node_id\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, split\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mval\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/michaelcummins/ADMM/experiments/cifar.ipynb#W1sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m b\u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/michaelcummins/ADMM/experiments/cifar.ipynb#W1sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/disop/lib/python3.10/site-packages/flwr_datasets/federated_dataset.py:135\u001b[0m, in \u001b[0;36mFederatedDataset.load_partition\u001b[0;34m(self, node_id, split)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_if_no_split_keyword_possible()\n\u001b[1;32m    134\u001b[0m     split \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_partitioners\u001b[39m.\u001b[39mkeys())[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> 135\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_if_split_present(split)\n\u001b[1;32m    136\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_if_split_possible_to_federate(split)\n\u001b[1;32m    137\u001b[0m partitioner: Partitioner \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_partitioners[split]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/disop/lib/python3.10/site-packages/flwr_datasets/federated_dataset.py:170\u001b[0m, in \u001b[0;36mFederatedDataset._check_if_split_present\u001b[0;34m(self, split)\u001b[0m\n\u001b[1;32m    168\u001b[0m available_splits \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset\u001b[39m.\u001b[39mkeys())\n\u001b[1;32m    169\u001b[0m \u001b[39mif\u001b[39;00m split \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m available_splits:\n\u001b[0;32m--> 170\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    171\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe given split: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00msplit\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is not present in the dataset\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms splits: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mavailable_splits\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    173\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The given split: 'val' is not present in the dataset's splits: '['train', 'test']'."
     ]
    }
   ],
   "source": [
    "from flwr_datasets import FederatedDataset\n",
    "from flwr_datasets.partitioner import ExponentialPartitioner, NaturalIdPartitioner, LinearPartitioner\n",
    "\n",
    "nodes = 3\n",
    "fds = FederatedDataset(dataset='cifar10', partitioners={'train': nodes, 'test': 1})\n",
    "partitions = [fds.load_partition(node_id=node, split='train') for node in range(nodes)]\n",
    "\n",
    "transforms = ToTensor()\n",
    "def apply_transforms(batch):\n",
    "  batch[\"img\"] = [transforms(img) for img in batch[\"img\"]]\n",
    "  return batch\n",
    "\n",
    "partitions_torch = [partition.with_transform(apply_transforms) for partition in partitions]\n",
    "\n",
    "print('Training partitions')\n",
    "for i, partition in enumerate(partitions):\n",
    "    print(f'\\nPartition {i}')\n",
    "    b= []\n",
    "    for i in range(10):\n",
    "        a = [1 for label in partition['label'] if label == i]\n",
    "        b.append(sum(a))\n",
    "        a = []\n",
    "    print(b)\n",
    "\n",
    "print('\\nTest partition\\n')\n",
    "partition = fds.load_partition(node_id=0, split='test')\n",
    "b= []\n",
    "for i in range(10):\n",
    "    a = [1 for label in partition['label'] if label == i]\n",
    "    b.append(sum(a))\n",
    "    a = []\n",
    "print(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
