{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from admm.agents import FedConsensus\n",
    "from admm.servers import EventADMM\n",
    "from admm.models import FCNet, CNN\n",
    "from admm.utils import average_params, split_dataset\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "cifar_trainset = datasets.CIFAR10(\n",
    "    root='./data/cifar10', train=True,\n",
    "    download=False, transform=cifar_transform\n",
    ")\n",
    "cifar_testset = datasets.CIFAR10(\n",
    "    root='./data/cifar10', train=False,\n",
    "    download=False, transform=cifar_transform\n",
    ")\n",
    "\n",
    "mnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)), transforms.Lambda(lambda x: torch.flatten(x))\n",
    "])\n",
    "\n",
    "mnist_trainset = datasets.MNIST(\n",
    "    root='./data/mnist_data', train=True,\n",
    "    download=False, transform=mnist_transform\n",
    ")\n",
    "mnist_testset = datasets.MNIST(\n",
    "    root='./data/mnist_data', train=False,\n",
    "    download=False, transform=mnist_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 0 distribution: [  21.  273.   72. 1951.   34.  607. 4135.  168.  204.  387.] - num_samples = 7852.0\n",
      "Dataset 1 distribution: [719. 530. 445. 127.  20.  40.   2.   1.  74. 231.] - num_samples = 2189.0\n",
      "Dataset 2 distribution: [  84.   32.  288.  266.    5. 2204.   32.  520.  903.  114.] - num_samples = 4448.0\n",
      "Dataset 3 distribution: [  10. 1275.   77.  646.   51.    9.   11.   54. 1657.  274.] - num_samples = 4064.0\n",
      "Dataset 4 distribution: [ 189.  226. 3712.   87.    6.  203.  118.   43.  218.   80.] - num_samples = 4882.0\n",
      "Dataset 5 distribution: [  29. 3156.   57.   87.    9.  505.  649. 3178.  171. 3505.] - num_samples = 11346.0\n",
      "Dataset 6 distribution: [  28.  357. 1217. 1295. 4325.  325.   40.   20.  180. 1019.] - num_samples = 8806.0\n",
      "Dataset 7 distribution: [ 462.  136.    5.   14.   14.  137.    5. 1956.  771.  213.] - num_samples = 3713.0\n",
      "Dataset 8 distribution: [4.309e+03 4.210e+02 3.000e+00 5.440e+02 1.126e+03 6.900e+01 2.000e+00\n",
      " 7.300e+01 1.638e+03 4.800e+01] - num_samples = 8233.0\n",
      "Dataset 9 distribution: [  67.  331.   75. 1110.  247. 1318.  919.  247.   31.   72.] - num_samples = 4417.0\n",
      "Validation dataset 9 distribution: [1162. 1336. 1168. 1271. 1127. 1100. 1177. 1276. 1139. 1244.] - num_samples = 12000.0\n",
      "torch.Size([1, 784])\n"
     ]
    }
   ],
   "source": [
    "from datset_preperation import _partition_data\n",
    "\n",
    "train_dataset, val_dataset, _ = split_dataset(dataset=mnist_trainset, train_ratio=0.8, val_ratio=0.2)\n",
    "\n",
    "trainsets = _partition_data(\n",
    "    num_clients=10,\n",
    "    iid=False,\n",
    "    balance=False,\n",
    "    power_law=True,\n",
    "    seed=42,\n",
    "    trainset=train_dataset.dataset\n",
    ")\n",
    "\n",
    "for i, dataset in enumerate(trainsets):\n",
    "    labels = np.zeros(10)\n",
    "    loader = DataLoader(dataset, batch_size=1)\n",
    "    for data, target in loader:\n",
    "        labels[target.item()] += 1\n",
    "    print(f'Dataset {i} distribution: {labels} - num_samples = {labels.sum()}')\n",
    "\n",
    "labels = np.zeros(10)\n",
    "loader = DataLoader(val_dataset, batch_size=1)\n",
    "for data, target in loader:\n",
    "    labels[target.item()] += 1\n",
    "print(f'Validation dataset {i} distribution: {labels} - num_samples = {labels.sum()}')\n",
    "\n",
    "oader = DataLoader(val_dataset, batch_size=10)\n",
    "for data, target in loader:\n",
    "    print(data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loaders = [DataLoader(dataset, batch_size=batch_size, shuffle=True) for dataset in trainsets]\n",
    "test_loader = DataLoader(mnist_testset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Comm frequency: 1.000, agent 0: 0.65, agent 1: 0.69, agent 2: 0.70, agent 3: 0.69, agent 4: 0.68, agent 5: 0.69, agent 6: 0.69, agent 7: 0.69, agent 8: 0.67, agent 9: 0.68:  27%|██▋       | 27/100 [09:48<24:30, 20.15s/it] "
     ]
    }
   ],
   "source": [
    "deltas = [0.001, 0.01, 0.1, 1, 10]\n",
    "rho = 0.01\n",
    "t_max = 100\n",
    "# loaders = [digit_1_train_loader_fc, digit_2_train_loader_fc]\n",
    "loaders = train_loaders\n",
    "device = 'cpu'\n",
    "\n",
    "acc_per_delta = np.zeros((len(deltas), t_max))\n",
    "rate_per_delta = np.zeros((len(deltas), t_max))\n",
    "loads = []\n",
    "test_accs = []\n",
    "\n",
    "print(f'N = {len(loaders)}')\n",
    "for i, delta in enumerate(deltas):\n",
    "    \n",
    "    agents = [\n",
    "        FedConsensus(\n",
    "            N=len(loaders),\n",
    "            delta=0,\n",
    "            rho=delta,\n",
    "            model=FCNet(in_channels=784, hidden1=200, hidden2=None, out_channels=10),\n",
    "            loss=nn.CrossEntropyLoss(),\n",
    "            train_loader=loader,\n",
    "            classification=True,\n",
    "            epochs=2,\n",
    "            device=device\n",
    "        ) for loader in loaders\n",
    "    ]\n",
    "\n",
    "    # Broadcast average to all agents and check if equal\n",
    "    for agent in agents:\n",
    "        agent.primal_avg = average_params([agent.model.parameters() for agent in agents])\n",
    "    for param1, param2 in zip(agents[0].primal_avg, agents[1].primal_avg):\n",
    "        if not torch.equal(param1, param2): raise ValueError(\"Averaged params aren't equal\")\n",
    "\n",
    "    # Run consensus algorithm\n",
    "    server = EventADMM(clients=agents, t_max=t_max)\n",
    "    server.spin(loader=val_loader)\n",
    "    \n",
    "    # For plotting purposes\n",
    "    acc_per_delta[i,:] = server.val_accs\n",
    "    rate_per_delta[i,:] = server.rates\n",
    "    loads.append(server.comm)\n",
    "    accs = server.validate(loader=test_loader)\n",
    "    test_accs.append(sum(accs)/len(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = range(t_max)\n",
    "# Plot accuracies\n",
    "for acc, delta in zip(acc_per_delta, deltas):\n",
    "    plt.plot(T, acc, label=f'rho={delta}')\n",
    "plt.legend(loc='center right', bbox_to_anchor=(1.3, 0.5))\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Set Accuracy - Fully Connected - Learning Rate = 0.001')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rate, delta in zip(rate_per_delta, deltas):\n",
    "    plt.plot(T, rate, label=f'delta={delta}')\n",
    "plt.legend(loc='center right', bbox_to_anchor=(1.3, 0.5))\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Rate')\n",
    "plt.title('Communication Rate - Fully Connected')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for load, acc, delta in zip(loads, test_accs, deltas):\n",
    "    plt.plot(acc, load, label=f'delta={delta}', marker='o')\n",
    "plt.legend(loc='center right', bbox_to_anchor=(1.3, 0.5))\n",
    "plt.xlabel('Test Error')\n",
    "plt.ylabel('Communication Load')\n",
    "plt.title('Fully Connected')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training partitions\n",
      "\n",
      "Partition 0\n",
      "[1705, 1605, 1682, 1628, 1669, 1683, 1674, 1692, 1643, 1686]\n",
      "\n",
      "Partition 1\n",
      "[1599, 1723, 1659, 1729, 1670, 1647, 1679, 1644, 1653, 1664]\n",
      "\n",
      "Partition 2\n",
      "[1696, 1672, 1659, 1643, 1661, 1670, 1647, 1664, 1704, 1650]\n",
      "\n",
      "Test partition\n",
      "\n",
      "[1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]\n",
      "\n",
      "Validation partition\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The given split: 'val' is not present in the dataset's splits: '['train', 'test']'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/michaelcummins/ADMM/experiments/cifar.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/michaelcummins/ADMM/experiments/cifar.ipynb#W1sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mprint\u001b[39m(b)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/michaelcummins/ADMM/experiments/cifar.ipynb#W1sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mValidation partition\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/michaelcummins/ADMM/experiments/cifar.ipynb#W1sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m partition \u001b[39m=\u001b[39m fds\u001b[39m.\u001b[39;49mload_partition(node_id\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, split\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mval\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/michaelcummins/ADMM/experiments/cifar.ipynb#W1sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m b\u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/michaelcummins/ADMM/experiments/cifar.ipynb#W1sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/disop/lib/python3.10/site-packages/flwr_datasets/federated_dataset.py:135\u001b[0m, in \u001b[0;36mFederatedDataset.load_partition\u001b[0;34m(self, node_id, split)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_if_no_split_keyword_possible()\n\u001b[1;32m    134\u001b[0m     split \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_partitioners\u001b[39m.\u001b[39mkeys())[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> 135\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_if_split_present(split)\n\u001b[1;32m    136\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_if_split_possible_to_federate(split)\n\u001b[1;32m    137\u001b[0m partitioner: Partitioner \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_partitioners[split]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/disop/lib/python3.10/site-packages/flwr_datasets/federated_dataset.py:170\u001b[0m, in \u001b[0;36mFederatedDataset._check_if_split_present\u001b[0;34m(self, split)\u001b[0m\n\u001b[1;32m    168\u001b[0m available_splits \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset\u001b[39m.\u001b[39mkeys())\n\u001b[1;32m    169\u001b[0m \u001b[39mif\u001b[39;00m split \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m available_splits:\n\u001b[0;32m--> 170\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    171\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe given split: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00msplit\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is not present in the dataset\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms splits: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mavailable_splits\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    173\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The given split: 'val' is not present in the dataset's splits: '['train', 'test']'."
     ]
    }
   ],
   "source": [
    "from flwr_datasets import FederatedDataset\n",
    "from flwr_datasets.partitioner import ExponentialPartitioner, NaturalIdPartitioner, LinearPartitioner\n",
    "\n",
    "nodes = 3\n",
    "fds = FederatedDataset(dataset='cifar10', partitioners={'train': nodes, 'test': 1})\n",
    "partitions = [fds.load_partition(node_id=node, split='train') for node in range(nodes)]\n",
    "\n",
    "transforms = ToTensor()\n",
    "def apply_transforms(batch):\n",
    "  batch[\"img\"] = [transforms(img) for img in batch[\"img\"]]\n",
    "  return batch\n",
    "\n",
    "partitions_torch = [partition.with_transform(apply_transforms) for partition in partitions]\n",
    "\n",
    "print('Training partitions')\n",
    "for i, partition in enumerate(partitions):\n",
    "    print(f'\\nPartition {i}')\n",
    "    b= []\n",
    "    for i in range(10):\n",
    "        a = [1 for label in partition['label'] if label == i]\n",
    "        b.append(sum(a))\n",
    "        a = []\n",
    "    print(b)\n",
    "\n",
    "print('\\nTest partition\\n')\n",
    "partition = fds.load_partition(node_id=0, split='test')\n",
    "b= []\n",
    "for i in range(10):\n",
    "    a = [1 for label in partition['label'] if label == i]\n",
    "    b.append(sum(a))\n",
    "    a = []\n",
    "print(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
