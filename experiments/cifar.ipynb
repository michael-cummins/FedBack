{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from admm.agents import FedConsensus\n",
    "from admm.servers import EventADMM\n",
    "from admm.models import FCNet, CNN\n",
    "from admm.utils import average_params, split_dataset\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "cifar_trainset = datasets.CIFAR10(\n",
    "    root='./data/cifar10', train=True,\n",
    "    download=False, transform=cifar_transform\n",
    ")\n",
    "\n",
    "mnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "mnist_trainset = datasets.MNIST(\n",
    "    root='./data/mnist_data', train=True,\n",
    "    download=False, transform=mnist_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list\n"
     ]
    }
   ],
   "source": [
    "if cifar_trainset.targets is not torch.Tensor: print('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([23659, 15003, 15008,  ..., 49997,     1,  4754])\n",
      "Dataset 0 distribution: [4630. 3065.    0.    0.    0.    0.    0.    0.    0.    0.] - num_samples = 7695.0\n",
      "Dataset 1 distribution: [   0. 1934.   73.    0.    0.    0.    0.    0.    0.    0.] - num_samples = 2007.0\n",
      "Dataset 2 distribution: [   0.    0. 4926.   60.    0.    0.    0.    0.    0.    0.] - num_samples = 4986.0\n",
      "Dataset 3 distribution: [   0.    0.    0. 4939. 4801.    0.    0.    0.    0.    0.] - num_samples = 9740.0\n",
      "Dataset 4 distribution: [  0.   0.   0.   0. 198.  65.   0.   0.   0.   0.] - num_samples = 263.0\n",
      "Dataset 5 distribution: [   0.    0.    0.    0.    0. 4934.  179.    0.    0.    0.] - num_samples = 5113.0\n",
      "Dataset 6 distribution: [   0.    0.    0.    0.    0.    0. 4820.  540.    0.    0.] - num_samples = 5360.0\n",
      "Dataset 7 distribution: [   0.    0.    0.    0.    0.    0.    0. 4459.  925.    0.] - num_samples = 5384.0\n",
      "Dataset 8 distribution: [   0.    0.    0.    0.    0.    0.    0.    0. 4074. 4946.] - num_samples = 9020.0\n",
      "Dataset 9 distribution: [369.   0.   0.   0.   0.   0.   0.   0.   0.  53.] - num_samples = 422.0\n"
     ]
    }
   ],
   "source": [
    "from datset_preperation import _partition_data\n",
    "\n",
    "trainsets = _partition_data(\n",
    "    num_clients=10,\n",
    "    iid=False,\n",
    "    balance=False,\n",
    "    power_law=True,\n",
    "    seed=42,\n",
    "    trainset=cifar_trainset\n",
    ")\n",
    "\n",
    "for i, dataset in enumerate(trainsets):\n",
    "    labels = np.zeros(10)\n",
    "    loader = DataLoader(dataset, batch_size=1)\n",
    "    for data, target in loader:\n",
    "        labels[target.item()] += 1\n",
    "    print(f'Dataset {i} distribution: {labels} - num_samples = {labels.sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 0 distribution: [ 776. 4921.    0.    0.    0.    0.    0.    0.    0.    0.]\n",
      "Dataset 1 distribution: [   0. 1820. 4251.    0.    0.    0.    0.    0.    0.    0.]\n",
      "Dataset 2 distribution: [   0.    0. 1706.   50.    0.    0.    0.    0.    0.    0.]\n",
      "Dataset 3 distribution: [   0.    0.    0. 6080. 1833.    0.    0.    0.    0.    0.]\n",
      "Dataset 4 distribution: [   0.    0.    0.    0. 4008. 5210.    0.    0.    0.    0.]\n",
      "Dataset 5 distribution: [   0.    0.    0.    0.    0.  210. 5785.    0.    0.    0.]\n",
      "Dataset 6 distribution: [   0.    0.    0.    0.    0.    0.  132. 3427.    0.    0.]\n",
      "Dataset 7 distribution: [   0.    0.    0.    0.    0.    0.    0. 2837.  525.    0.]\n",
      "Dataset 8 distribution: [   0.    0.    0.    0.    0.    0.    0.    0. 5325. 2869.]\n",
      "Dataset 9 distribution: [5146.    0.    0.    0.    0.    0.    0.    0.    0. 3079.]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training partitions\n",
      "\n",
      "Partition 0\n",
      "[1705, 1605, 1682, 1628, 1669, 1683, 1674, 1692, 1643, 1686]\n",
      "\n",
      "Partition 1\n",
      "[1599, 1723, 1659, 1729, 1670, 1647, 1679, 1644, 1653, 1664]\n",
      "\n",
      "Partition 2\n",
      "[1696, 1672, 1659, 1643, 1661, 1670, 1647, 1664, 1704, 1650]\n",
      "\n",
      "Test partition\n",
      "\n",
      "[1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]\n",
      "\n",
      "Validation partition\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The given split: 'val' is not present in the dataset's splits: '['train', 'test']'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/michaelcummins/ADMM/experiments/cifar.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/michaelcummins/ADMM/experiments/cifar.ipynb#W1sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mprint\u001b[39m(b)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/michaelcummins/ADMM/experiments/cifar.ipynb#W1sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mValidation partition\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/michaelcummins/ADMM/experiments/cifar.ipynb#W1sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m partition \u001b[39m=\u001b[39m fds\u001b[39m.\u001b[39;49mload_partition(node_id\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, split\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mval\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/michaelcummins/ADMM/experiments/cifar.ipynb#W1sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m b\u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/michaelcummins/ADMM/experiments/cifar.ipynb#W1sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/disop/lib/python3.10/site-packages/flwr_datasets/federated_dataset.py:135\u001b[0m, in \u001b[0;36mFederatedDataset.load_partition\u001b[0;34m(self, node_id, split)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_if_no_split_keyword_possible()\n\u001b[1;32m    134\u001b[0m     split \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_partitioners\u001b[39m.\u001b[39mkeys())[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> 135\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_if_split_present(split)\n\u001b[1;32m    136\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_if_split_possible_to_federate(split)\n\u001b[1;32m    137\u001b[0m partitioner: Partitioner \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_partitioners[split]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/disop/lib/python3.10/site-packages/flwr_datasets/federated_dataset.py:170\u001b[0m, in \u001b[0;36mFederatedDataset._check_if_split_present\u001b[0;34m(self, split)\u001b[0m\n\u001b[1;32m    168\u001b[0m available_splits \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset\u001b[39m.\u001b[39mkeys())\n\u001b[1;32m    169\u001b[0m \u001b[39mif\u001b[39;00m split \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m available_splits:\n\u001b[0;32m--> 170\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    171\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe given split: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00msplit\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is not present in the dataset\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms splits: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mavailable_splits\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    173\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The given split: 'val' is not present in the dataset's splits: '['train', 'test']'."
     ]
    }
   ],
   "source": [
    "from flwr_datasets import FederatedDataset\n",
    "from flwr_datasets.partitioner import ExponentialPartitioner, NaturalIdPartitioner, LinearPartitioner\n",
    "\n",
    "nodes = 3\n",
    "fds = FederatedDataset(dataset='cifar10', partitioners={'train': nodes, 'test': 1})\n",
    "partitions = [fds.load_partition(node_id=node, split='train') for node in range(nodes)]\n",
    "\n",
    "transforms = ToTensor()\n",
    "def apply_transforms(batch):\n",
    "  batch[\"img\"] = [transforms(img) for img in batch[\"img\"]]\n",
    "  return batch\n",
    "\n",
    "partitions_torch = [partition.with_transform(apply_transforms) for partition in partitions]\n",
    "\n",
    "print('Training partitions')\n",
    "for i, partition in enumerate(partitions):\n",
    "    print(f'\\nPartition {i}')\n",
    "    b= []\n",
    "    for i in range(10):\n",
    "        a = [1 for label in partition['label'] if label == i]\n",
    "        b.append(sum(a))\n",
    "        a = []\n",
    "    print(b)\n",
    "\n",
    "print('\\nTest partition\\n')\n",
    "partition = fds.load_partition(node_id=0, split='test')\n",
    "b= []\n",
    "for i in range(10):\n",
    "    a = [1 for label in partition['label'] if label == i]\n",
    "    b.append(sum(a))\n",
    "    a = []\n",
    "print(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
